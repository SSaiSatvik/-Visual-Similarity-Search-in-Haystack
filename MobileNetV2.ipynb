{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:42:28.104766Z",
     "iopub.status.busy": "2024-11-11T17:42:28.104372Z",
     "iopub.status.idle": "2024-11-11T17:42:42.325653Z",
     "shell.execute_reply": "2024-11-11T17:42:42.324681Z",
     "shell.execute_reply.started": "2024-11-11T17:42:28.104722Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from datasets import load_dataset\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:42:42.327821Z",
     "iopub.status.busy": "2024-11-11T17:42:42.327240Z",
     "iopub.status.idle": "2024-11-11T17:43:10.839440Z",
     "shell.execute_reply": "2024-11-11T17:43:10.838255Z",
     "shell.execute_reply.started": "2024-11-11T17:42:42.327787Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5aaef5ba72d1483a9f27a470d5f6c81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.90k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e552087f1b4b4154abb1fe61b34dcc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json:   0%|          | 0.00/3.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5c40d02c7b64ed8a987a881bf4b1da3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-1359597a978bc4fa.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f58ce89ea9b404a8754ca849ecd7768",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-70d52db3c749a935.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b8dcbd4a10b41bfb4f6fb5932d9c609",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9218a96ae7264d8aa4ecdc51cbd4f67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e26959129d344dfaae787d9df4648abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "506fde93737a4166bcd6d69a437cc761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the Tiny ImageNet dataset\n",
    "cache_dir = \"/kaggle/working/tiny-imagenet\"  # Kaggle's working directory\n",
    "\n",
    "# Load the dataset (it will download if not already present)\n",
    "ds = load_dataset(\"zh-plus/tiny-imagenet\", cache_dir=cache_dir)\n",
    "\n",
    "# Filter the dataset to include only the first 20 classes for the training set\n",
    "filtered_ds_train = ds['train'].filter(lambda example: example['label'] < 20)\n",
    "filtered_ds_valid = ds['valid'].filter(lambda example: example['label'] < 20)  # Use valid as validation set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-11T17:43:10.841042Z",
     "iopub.status.busy": "2024-11-11T17:43:10.840736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "\u001b[1m9406464/9406464\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Skipping image with shape (224, 224)\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1731347051.463288      84 service.cc:145] XLA service 0x7be6c4004b90 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1731347051.463348      84 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/156\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57:38\u001b[0m 46s/step - loss: 3.1092 - sparse_categorical_accuracy: 0.0938"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1731347073.766939      84 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 266ms/step - loss: 1.7368 - sparse_categorical_accuracy: 0.5047\n",
      "Epoch 1: val_sparse_categorical_accuracy improved from -inf to 0.12738, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 319ms/step - loss: 1.7346 - sparse_categorical_accuracy: 0.5053 - val_loss: 13.1564 - val_sparse_categorical_accuracy: 0.1274\n",
      "Epoch 2/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.8111 - sparse_categorical_accuracy: 0.7671\n",
      "Epoch 2: val_sparse_categorical_accuracy improved from 0.12738 to 0.15948, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 152ms/step - loss: 0.8112 - sparse_categorical_accuracy: 0.7670 - val_loss: 14.2047 - val_sparse_categorical_accuracy: 0.1595\n",
      "Epoch 3/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.5887 - sparse_categorical_accuracy: 0.8229\n",
      "Epoch 3: val_sparse_categorical_accuracy did not improve from 0.15948\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.5888 - sparse_categorical_accuracy: 0.8228 - val_loss: 17.7001 - val_sparse_categorical_accuracy: 0.1585\n",
      "Epoch 4/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.4284 - sparse_categorical_accuracy: 0.8681\n",
      "Epoch 4: val_sparse_categorical_accuracy did not improve from 0.15948\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.4288 - sparse_categorical_accuracy: 0.8680 - val_loss: 23.4826 - val_sparse_categorical_accuracy: 0.0522\n",
      "Epoch 5/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.3432 - sparse_categorical_accuracy: 0.8927\n",
      "Epoch 5: val_sparse_categorical_accuracy did not improve from 0.15948\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.3434 - sparse_categorical_accuracy: 0.8926 - val_loss: 14.7140 - val_sparse_categorical_accuracy: 0.1374\n",
      "Epoch 6/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.2841 - sparse_categorical_accuracy: 0.9164\n",
      "Epoch 6: val_sparse_categorical_accuracy did not improve from 0.15948\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.2844 - sparse_categorical_accuracy: 0.9163 - val_loss: 21.2745 - val_sparse_categorical_accuracy: 0.0802\n",
      "Epoch 7/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - loss: 0.2803 - sparse_categorical_accuracy: 0.9110\n",
      "Epoch 7: val_sparse_categorical_accuracy improved from 0.15948 to 0.23069, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 153ms/step - loss: 0.2804 - sparse_categorical_accuracy: 0.9110 - val_loss: 9.3447 - val_sparse_categorical_accuracy: 0.2307\n",
      "Epoch 8/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9222\n",
      "Epoch 8: val_sparse_categorical_accuracy did not improve from 0.23069\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.2396 - sparse_categorical_accuracy: 0.9221 - val_loss: 10.5309 - val_sparse_categorical_accuracy: 0.2126\n",
      "Epoch 9/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.2125 - sparse_categorical_accuracy: 0.9309\n",
      "Epoch 9: val_sparse_categorical_accuracy did not improve from 0.23069\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.2126 - sparse_categorical_accuracy: 0.9309 - val_loss: 18.4863 - val_sparse_categorical_accuracy: 0.0772\n",
      "Epoch 10/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1729 - sparse_categorical_accuracy: 0.9462\n",
      "Epoch 10: val_sparse_categorical_accuracy did not improve from 0.23069\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9461 - val_loss: 18.9078 - val_sparse_categorical_accuracy: 0.1103\n",
      "Epoch 11/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.9349\n",
      "Epoch 11: val_sparse_categorical_accuracy did not improve from 0.23069\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.9349 - val_loss: 10.6081 - val_sparse_categorical_accuracy: 0.1645\n",
      "Epoch 12/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1516 - sparse_categorical_accuracy: 0.9530\n",
      "Epoch 12: val_sparse_categorical_accuracy improved from 0.23069 to 0.27382, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 152ms/step - loss: 0.1517 - sparse_categorical_accuracy: 0.9530 - val_loss: 7.8450 - val_sparse_categorical_accuracy: 0.2738\n",
      "Epoch 13/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1198 - sparse_categorical_accuracy: 0.9646\n",
      "Epoch 13: val_sparse_categorical_accuracy did not improve from 0.27382\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.1200 - sparse_categorical_accuracy: 0.9645 - val_loss: 14.0000 - val_sparse_categorical_accuracy: 0.1565\n",
      "Epoch 14/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1493 - sparse_categorical_accuracy: 0.9503\n",
      "Epoch 14: val_sparse_categorical_accuracy did not improve from 0.27382\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.1494 - sparse_categorical_accuracy: 0.9503 - val_loss: 10.3025 - val_sparse_categorical_accuracy: 0.2076\n",
      "Epoch 15/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1604 - sparse_categorical_accuracy: 0.9500\n",
      "Epoch 15: val_sparse_categorical_accuracy did not improve from 0.27382\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.1604 - sparse_categorical_accuracy: 0.9500 - val_loss: 10.5815 - val_sparse_categorical_accuracy: 0.1645\n",
      "Epoch 16/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1379 - sparse_categorical_accuracy: 0.9544\n",
      "Epoch 16: val_sparse_categorical_accuracy did not improve from 0.27382\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 148ms/step - loss: 0.1379 - sparse_categorical_accuracy: 0.9544 - val_loss: 7.7973 - val_sparse_categorical_accuracy: 0.1565\n",
      "Epoch 17/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1179 - sparse_categorical_accuracy: 0.9653\n",
      "Epoch 17: val_sparse_categorical_accuracy improved from 0.27382 to 0.32397, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 152ms/step - loss: 0.1179 - sparse_categorical_accuracy: 0.9653 - val_loss: 6.6966 - val_sparse_categorical_accuracy: 0.3240\n",
      "Epoch 18/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1057 - sparse_categorical_accuracy: 0.9684\n",
      "Epoch 18: val_sparse_categorical_accuracy did not improve from 0.32397\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 149ms/step - loss: 0.1059 - sparse_categorical_accuracy: 0.9683 - val_loss: 9.3463 - val_sparse_categorical_accuracy: 0.2427\n",
      "Epoch 19/25\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9645\n",
      "Epoch 19: val_sparse_categorical_accuracy improved from 0.32397 to 0.38315, saving model to /kaggle/working/mobilenetv2_best_model.keras\n",
      "\u001b[1m156/156\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 152ms/step - loss: 0.1133 - sparse_categorical_accuracy: 0.9645 - val_loss: 5.6879 - val_sparse_categorical_accuracy: 0.3831\n",
      "Epoch 20/25\n",
      "\u001b[1m 91/156\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 144ms/step - loss: 0.0890 - sparse_categorical_accuracy: 0.9709"
     ]
    }
   ],
   "source": [
    "# Load the MobileNetV2 model pre-trained on ImageNet, without the top layers\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add a custom dense layer to create embeddings of a specific size\n",
    "embedding_size = 64\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(embedding_size, activation='relu')(x)\n",
    "predictions = Dense(20, activation='softmax')(x)  # Only 20 classes\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model for training\n",
    "model.compile(optimizer=Adam(),\n",
    "              loss=SparseCategoricalCrossentropy(),\n",
    "              metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image):\n",
    "    image = Image.fromarray(image)\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image)\n",
    "    if image.shape == (224, 224, 3):  # Ensure the image has 3 channels\n",
    "        return preprocess_input(image)\n",
    "    else:\n",
    "        print(f\"Skipping image with shape {image.shape}\")\n",
    "        return None\n",
    "\n",
    "# Function to prepare data\n",
    "def prepare_data(dataset):\n",
    "    images, labels = [], []\n",
    "    for example in dataset:\n",
    "        processed_image = preprocess_image(np.array(example['image']))\n",
    "        if processed_image is not None:\n",
    "            images.append(processed_image)\n",
    "            labels.append(example['label'])\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Prepare the training data\n",
    "train_images, train_labels = prepare_data(filtered_ds_train)\n",
    "\n",
    "# Prepare the validation data (using the valid dataset)\n",
    "valid_images, valid_labels = prepare_data(filtered_ds_valid)\n",
    "\n",
    "# Define a callback to save the model with the best validation accuracy\n",
    "checkpoint = ModelCheckpoint(\n",
    "    '/kaggle/working/mobilenetv2_best_model.keras',  # Path where the best model will be saved\n",
    "    monitor='val_sparse_categorical_accuracy',  # Monitor validation accuracy\n",
    "    save_best_only=True,  # Save only when the validation accuracy improves\n",
    "    mode='max',  # 'max' means we want to maximize the validation accuracy\n",
    "    verbose=1  # To print when a new best model is saved\n",
    ")\n",
    "\n",
    "# Train the model on the Kaggle GPU with validation data, using the callback\n",
    "history = model.fit(\n",
    "    train_images, train_labels, \n",
    "    epochs=300, batch_size=64, \n",
    "    validation_data=(valid_images, valid_labels),\n",
    "    callbacks=[checkpoint]  # Include the checkpoint callback in the training process\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model locally on Kaggle's output directory\n",
    "model_without_last_layer = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "model_without_last_layer.save('/kaggle/working/mobilenetv2_embeddings_model.h5')\n",
    "\n",
    "# Optionally, plot the training and validation accuracy and loss\n",
    "plt.plot(history.history['sparse_categorical_accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save the model locally on Kaggle's output directory\n",
    "model_without_last_layer = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "model_without_last_layer.save('/kaggle/working/mobilenetv2_embeddings_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30787,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
